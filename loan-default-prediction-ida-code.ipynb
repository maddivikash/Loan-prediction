{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **MS4610 - Introduction to Data Analytics - Final Course Project**\n\n__Problem Statement:__ <br>\n     The project deals in predicting the nature of Loan given (default or non-default) by an organization using the training dataset of given features like customer's age, income, expenses, occupation type and also some of the metrics calculated by the organization.\n\n__Data Description:__ <br>\n\n| Variable | Description | \n|------|------|\n| ID | Unique Loan Identifier | \n| Loan Type | A/ B | \n| Occupation Type | customer's occupation(X/Y/Z) | \n| Income | Annual income of customer | \n| Expense | Annual expense of customer | \n| Age | 0 for customer's age below 50 / 1 for above 50 | \n| Score 1 | Customer Metric | \n| Score 2 | Customer Metric | \n| Score 3 | Customer Metric | \n| Score 4 | Customer Metric | \n| Score 5 | Customer Metric | \n| Label | 0 for non-default/ 1 for default type | \n\n"},{"metadata":{},"cell_type":"markdown","source":"## A. Data Importing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"############## Importing Libraries ##################\nimport pandas as pd \nimport matplotlib.pyplot as py\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We imported the data by uploading the three csv files given i.e train_x.csv, train_y.csv and test_x.csv (in kaggle) and naming them together as loan-dataset. The total input file size is 9.7 MB."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":1.697277,"end_time":"2020-12-07T06:52:49.677777","exception":false,"start_time":"2020-12-07T06:52:47.9805","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"########## reading training dataset ###############\ntrain_data = pd.read_csv('../input/loan-dataset/train_x.csv')\ntrain_data.info()\n\n########## reading training labels ################\ntrain_label_data=pd.read_csv('../input/loan-dataset/train_y.csv')\ntrain_label_data.info()\n\ntrain_data = train_data.merge(train_label_data,on = 'ID')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.06875,"end_time":"2020-12-07T06:52:49.776395","exception":false,"start_time":"2020-12-07T06:52:49.707645","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B. Data Visualization:"},{"metadata":{"papermill":{"duration":0.0649,"end_time":"2020-12-07T06:52:49.870912","exception":false,"start_time":"2020-12-07T06:52:49.806012","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"###### leaving out unlabelled rows ###########\nnew_train_data = train_data[train_data.Label.notnull()]  \nnew_train_data.head(7)\n\n###### calculating percentage of data dropped #######\nlength = len(train_data.ID)\nlength_new = len(new_train_data.ID)\nper = (1 - length_new/length) *100\nprint(\"Percent of data dropped is \",per)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.259818,"end_time":"2020-12-07T06:52:50.160902","exception":false,"start_time":"2020-12-07T06:52:49.901084","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"###### checking for skewdness in labels in new_train_dataset ######\nsns.countplot(x = \"Label\",data = new_train_data)\n\nl = len(new_train_data.Label)\ns = new_train_data.Label.sum()\nprint(s)\npercent = (s/l)*100\nprint(\"Percentage of labels which are default is\",percent)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interested_columns=['Loan type', 'Occupation type','Age' ]\nfor col in interested_columns:\n    categorical_bin = pd.crosstab(new_train_data[col],new_train_data['Label'])\n    categorical_bin.div(categorical_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\n    py.xlabel(f'{col}')\n    P = py.ylabel('Percentage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Pearson correlation coefficient ###########\ncorr = new_train_data.corr(method = 'pearson')\nf, ax = py.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(10, 275, as_cmap=True)\n\nsns.heatmap(corr, cmap=cmap, square=True,\n            linewidths=0.5, cbar_kws={\"shrink\": 0.5}, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":1.256455,"end_time":"2020-12-07T06:52:51.449319","exception":false,"start_time":"2020-12-07T06:52:50.192864","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"########### Box plot of numerical columns ###########\nnumerical_columns= ['Expense','Income', 'Score1','Score2','Score3','Score4', 'Score5']\n\n\nfig,axes = py.subplots(3,3,figsize=(20,14))\nfor idx,cat_col in enumerate(numerical_columns):\n     row,col = idx//3,idx%3\n     sns.boxplot(y=cat_col,data=train_data,x='Label',ax=axes[row,col])\n\nprint(train_data[numerical_columns].describe())\npy.subplots_adjust(hspace=0.5)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Plotting all the pairs of numerical columns ###########\ninterested_columns = ['Expense','Income', 'Score1','Score2','Score3','Score4','Score5','Label']\nsns.pairplot(new_train_data[interested_columns][:5000],hue='Label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Observations:__<br>\n1. There is an imbalance in the Label column\n2. There is a high correlation between Score5 and Expense\n\n__Solutions:__<br>\n1. Using SMOTE to balance the dataset\n2. As there are not much features, the correlated features are not removed"},{"metadata":{},"cell_type":"markdown","source":"## C. Data Pre-processing:"},{"metadata":{"papermill":{"duration":0.054276,"end_time":"2020-12-07T06:52:51.537357","exception":false,"start_time":"2020-12-07T06:52:51.483081","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"########## defining numerical and categorical columns ###########\ncategorical_columns=['ID','Loan type', 'Occupation type','Age' ]\nnumerical_columns= ['ID','Expense','Income', 'Score1','Score2','Score3','Score4', 'Score5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_train_data.drop(columns='Label')\ny = new_train_data['Label']\n\n############# Mode filling for categorical columns ################\nfrom sklearn.impute import SimpleImputer\nimp= SimpleImputer(strategy = 'most_frequent')\nX_categorical = imp.fit_transform(X[categorical_columns])\nX_categorical = pd.DataFrame(X_categorical,columns=categorical_columns)\n\n############# Mean filling for numerical columns ##################\nimp= SimpleImputer(strategy = 'mean')\nX_numerical = imp.fit_transform(X[numerical_columns])\nX_numerical = pd.DataFrame(X_numerical,columns=numerical_columns)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.133974,"end_time":"2020-12-07T06:52:52.995509","exception":false,"start_time":"2020-12-07T06:52:52.861535","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X = X_numerical.merge(X_categorical,on=\"ID\")\nX = X.drop(columns = 'ID')\n\n############# Encoding Categorical features #################\nX = pd.get_dummies(X,drop_first=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.089798,"end_time":"2020-12-07T06:52:53.119457","exception":false,"start_time":"2020-12-07T06:52:53.029659","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"####### Using SMOTE for making the data set balanced #######\nfrom imblearn.over_sampling import SMOTE\nsmk = SMOTE(random_state=0)\nX_new,y_new = smk.fit_sample(X,y)\nlen(y_new)\n\n####### checking whether dataset became balanced or not ######\nl = len(y_new)\ns = y_new.sum()\nprint(s)\npercent = (s/l)*100\nprint(\"Percentage of labels which are default after balancing the data set is\",percent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Plotting all the pairs of numerical columns after SMOTE ###########\ninterested_columns = ['Expense','Income', 'Score1','Score2','Score3','Score4','Score5','Label']\nsmote_df = pd.concat([X_new, y_new], axis=1)\nsmote_df = smote_df.sample(frac=1).reset_index(drop=True)\nsns.pairplot(smote_df[interested_columns][:5000],hue='Label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fifty percent of data are labelled as default after applying SMOTE. The data-set became balanced."},{"metadata":{},"cell_type":"markdown","source":"## D. Model Training:"},{"metadata":{"papermill":{"duration":0.035935,"end_time":"2020-12-07T06:52:53.936919","exception":false,"start_time":"2020-12-07T06:52:53.900984","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Model 1 - Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":" Logistic Regression model is sensitive to variation in the dataset, so standadizing data has to be done."},{"metadata":{},"cell_type":"markdown","source":"### Standardizing Numerical columns in Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns= ['Expense','Income', 'Score1','Score2','Score3','Score4', 'Score5']\ncategorical_columns =['Loan type_B','Occupation type_Y','Occupation type_Z','Age_1.0']\n\nX_standard =pd.DataFrame([])\n\n################## Standarizing values only for numerical columns################\nfrom sklearn.preprocessing import StandardScaler\n\nX_standard[numerical_columns] =pd.DataFrame(StandardScaler().fit_transform(X_new[numerical_columns]))\n\n################## combining categorical columns ################\nX_standard[categorical_columns]=X_new[categorical_columns]\nX_standard.info()\nX_standard.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Splitting data into Training and Test Data for the standardized data ###########\nfrom sklearn.model_selection import train_test_split\nX_train_encoded,X_test_encoded,y_train,y_test = train_test_split(X_standard,y_new,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":20.254615,"end_time":"2020-12-07T06:53:14.229885","exception":false,"start_time":"2020-12-07T06:52:53.97527","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn.model_selection import cross_val_predict\n\ntrain_accuracies = []\ntrain_f1_scores = []\ntest_accuracies = []\ntest_f1_scores = []\nthresholds = []\n\n#Using different threshold values and finding the accuracy of the model\nfor thresh in np.arange(0.1,0.9,0.1): ## Sweeping from threshold of 0.1 to 0.9\n    logreg_clf = LogisticRegression(solver='liblinear')\n    logreg_clf.fit(X_train_encoded,y_train)\n    \n    y_pred_train_thresh = logreg_clf.predict_proba(X_train_encoded)[:,1]\n    y_pred_train = (y_pred_train_thresh > thresh).astype(int)\n\n    train_acc = accuracy_score(y_train,y_pred_train)\n    train_f1 = f1_score(y_train,y_pred_train)\n    \n    y_pred_test_thresh = logreg_clf.predict_proba(X_test_encoded)[:,1]\n    y_pred_test = (y_pred_test_thresh > thresh).astype(int) \n    \n    test_acc = accuracy_score(y_test,y_pred_test)\n    test_f1 = f1_score(y_test,y_pred_test)\n    \n    train_accuracies.append(train_acc)\n    train_f1_scores.append(train_f1)\n    test_accuracies.append(test_acc)\n    test_f1_scores.append(test_f1)\n    thresholds.append(thresh)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.423175,"end_time":"2020-12-07T06:53:14.689847","exception":false,"start_time":"2020-12-07T06:53:14.266672","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"Threshold_logreg = {\"Training Accuracy\": train_accuracies, \"Test Accuracy\": test_accuracies, \"Training F1\": train_f1_scores, \"Test F1\":test_f1_scores, \"Decision Threshold\": thresholds }\nThreshold_logreg_df = pd.DataFrame.from_dict(Threshold_logreg)\n\nplot_df = Threshold_logreg_df.melt('Decision Threshold',var_name='Metrics',value_name=\"Values\")\nfig,ax = py.subplots(figsize=(15,5))\nsns.pointplot(x=\"Decision Threshold\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Using the threshold value as 0.45"},{"metadata":{"papermill":{"duration":2.541904,"end_time":"2020-12-07T06:53:17.270472","exception":false,"start_time":"2020-12-07T06:53:14.728568","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# using a threshold of 0.45 for Log regression:\n\nlogreg_clf = LogisticRegression(solver='liblinear')\nlogreg_clf.fit(X_train_encoded,y_train)\n    \ny_pred_train_thresh = logreg_clf.predict_proba(X_train_encoded)[:,1]\ny_pred_train = (y_pred_train_thresh > 0.45).astype(int)\n\ntrain_acc = accuracy_score(y_train,y_pred_train)\ntrain_f1 = f1_score(y_train,y_pred_train)\n    \ny_pred_test_thresh = logreg_clf.predict_proba(X_test_encoded)[:,1]\ny_pred_test = (y_pred_test_thresh >0.45).astype(int) \n    \ntest_acc = accuracy_score(y_test,y_pred_test)\ntest_f1 = f1_score(y_test,y_pred_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results for Logistic Regression:"},{"metadata":{"papermill":{"duration":0.16599,"end_time":"2020-12-07T06:53:17.47774","exception":false,"start_time":"2020-12-07T06:53:17.31175","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"################# Training Data Results ######################\nprint(\"Training acc. is :\", train_acc)\nprint(\"Training f1 :\",train_f1)\npd.crosstab(y_train, y_pred_train, rownames=['Actual'], colnames=['Predicted'], margins=True)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.110815,"end_time":"2020-12-07T06:53:17.633627","exception":false,"start_time":"2020-12-07T06:53:17.522812","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"################## Test Data Results #################\nprint(\"Test acc. is :\", test_acc)\nprint(\"Test f1 :\",test_f1)\npd.crosstab(y_test, y_pred_test, rownames=['Actual'], colnames=['Predicted'], margins=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC curve for Logistic Regression:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_roc_curve\nax=py.gca()\nrfc=plot_roc_curve(logreg_clf,X_test_encoded,y_test,ax=ax,alpha=0.8)\npy.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_matrix = logreg_clf.coef_\nprint(coeff_matrix)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.041406,"end_time":"2020-12-07T06:53:17.71704","exception":false,"start_time":"2020-12-07T06:53:17.675634","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Model 2 - Decision Tree Classifier"},{"metadata":{},"cell_type":"markdown","source":"Decision tree classifier and Random Forest classifier is not sensitive to variation in the dataset. So, Test train split can be done to dataset without standardization (scaling) of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Splitting data into Training and Test Data ###########\nfrom sklearn.model_selection import train_test_split\nX_train_encoded,X_test_encoded,y_train,y_test = train_test_split(X_new,y_new,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":15.408422,"end_time":"2020-12-07T06:53:33.167239","exception":false,"start_time":"2020-12-07T06:53:17.758817","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,f1_score\n\n\ntree_clf = DecisionTreeClassifier()\ntree_clf.fit(X_train_encoded,y_train)\ny_pred = tree_clf.predict(X_train_encoded)\nprint(\"Training Data Set Accuracy: \", accuracy_score(y_train,y_pred))\nprint(\"Training Data F1 Score \", f1_score(y_train,y_pred))\n\nprint(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train_encoded,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train_encoded,y_train,cv=5,scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.087731,"end_time":"2020-12-07T06:53:33.297396","exception":false,"start_time":"2020-12-07T06:53:33.209665","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"y_pred = tree_clf.predict(X_test_encoded)\nprint(\"Test Data Set Accuracy: \", accuracy_score(y_test,y_pred))\nprint(\"Test Data F1 Score \", f1_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" On observing the trained and test accuracy of the model, it can be seen that the trained model has been overfitted."},{"metadata":{"papermill":{"duration":220.906248,"end_time":"2020-12-07T06:57:14.246384","exception":false,"start_time":"2020-12-07T06:53:33.340136","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"########## tuning the depth parameter ##########\ntraining_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\ntree_depths = []\ntest_accuracy = []\ntest_val_accuracy =[]\ntest_val_f1 = []\ntest_f1 =[]\n\nfor depth in range(1,20):\n    tree_clf = DecisionTreeClassifier(max_depth=depth)\n    tree_clf.fit(X_train_encoded,y_train)\n    y_training_pred = tree_clf.predict(X_train_encoded)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(tree_clf,X_train_encoded,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(tree_clf,X_train_encoded,y_train,cv=5,scoring='accuracy').mean()\n    \n    y_test_pred_1 = tree_clf.predict(X_test_encoded)\n\n    training_acc_1 = accuracy_score(y_test,y_test_pred_1)\n    train_f1_1 = f1_score(y_test,y_test_pred_1)\n    val_mean_f1_1 = cross_val_score(tree_clf,X_test_encoded,y_test,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy_1 = cross_val_score(tree_clf,X_test_encoded,y_test,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    tree_depths.append(depth)\n    \n     \n    test_accuracy.append(training_acc_1)\n    test_val_accuracy.append(val_mean_accuracy_1)\n    test_f1.append(train_f1_1)\n    test_val_f1.append(val_mean_f1_1)\n    \n\nTuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths ,\"Test_val_f1\":test_val_f1 , \"Test_val_acc\":test_val_accuracy , \"Test_acc\":test_accuracy , \"Test_f1\":test_f1 }\nTuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n\nplot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\nfig,ax = py.subplots(figsize=(15,5))\nsns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.601634,"end_time":"2020-12-07T06:57:14.894226","exception":false,"start_time":"2020-12-07T06:57:14.292592","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"Tuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths }\nTuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n\nplot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\nfig,ax = py.subplots(figsize=(15,5))\nsns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.671008,"end_time":"2020-12-07T06:57:15.612659","exception":false,"start_time":"2020-12-07T06:57:14.941651","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"Tuning_Max_depth = {  \"Max_Depth\": tree_depths , \"Test_acc\":test_accuracy ,\"Test_val_acc\":test_val_accuracy,\"Test_f1\":test_f1 ,\"Test_val_f1\":test_val_f1  }\nTuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n\nplot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\nfig,ax = py.subplots(figsize=(15,5))\nsns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### From the graph plotted, depth = 8 seems to be a reasonable value, considering computational time limts and to reduce overfitting risks. "},{"metadata":{"papermill":{"duration":9.405541,"end_time":"2020-12-07T06:57:25.068553","exception":false,"start_time":"2020-12-07T06:57:15.663012","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# depth = 8\n\ntree_clf = DecisionTreeClassifier(max_depth =8)\ntree_clf.fit(X_train_encoded,y_train)\ny_pred = tree_clf.predict(X_train_encoded)\nprint(\"Training Data Set Accuracy: \", accuracy_score(y_train,y_pred))\nprint(\"Training Data F1 Score \", f1_score(y_train,y_pred))\n\nprint(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train_encoded,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train_encoded,y_train,cv=5,scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.153149,"end_time":"2020-12-07T06:57:25.277822","exception":false,"start_time":"2020-12-07T06:57:25.124673","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"pd.crosstab(y_train, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":1.905894,"end_time":"2020-12-07T06:57:27.235713","exception":false,"start_time":"2020-12-07T06:57:25.329819","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#testing decision tree clasifier of depth 8\ny_pred = tree_clf.predict(X_test_encoded)\nprint(\"Test Data Set Accuracy: \", accuracy_score(y_test,y_pred))\nprint(\"Test Data F1 Score \", f1_score(y_test,y_pred))\n\nprint(\"Validation Test Mean F1 Score: \",cross_val_score(tree_clf,X_test_encoded,y_test,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Test Mean Accuracy: \",cross_val_score(tree_clf,X_test_encoded,y_test,cv=5,scoring='accuracy').mean())\n\npd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ROC curve for Decision Tree classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_roc_curve\nax=py.gca()\nrfc=plot_roc_curve(tree_clf,X_test_encoded,y_test,ax=ax,alpha=0.8)\npy.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.056224,"end_time":"2020-12-07T06:57:27.459822","exception":false,"start_time":"2020-12-07T06:57:27.403598","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Model 3 - Random Forest"},{"metadata":{},"cell_type":"markdown","source":"No standardization (scaling) is done here, as stated before."},{"metadata":{"papermill":{"duration":31.576134,"end_time":"2020-12-07T06:57:59.090663","exception":false,"start_time":"2020-12-07T06:57:27.514529","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=100,max_depth=14,min_samples_leaf = 10, random_state = 42)\nrf_clf.fit(X_train_encoded,y_train)\ny_pred = rf_clf.predict(X_train_encoded)\nprint(\"Train F1 Score \", f1_score(y_train,y_pred))\nprint(\"Train Accuracy \", accuracy_score(y_train,y_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.151667,"end_time":"2020-12-07T06:57:59.298538","exception":false,"start_time":"2020-12-07T06:57:59.146871","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\npd.crosstab(y_train, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.626695,"end_time":"2020-12-07T06:58:00.092405","exception":false,"start_time":"2020-12-07T06:57:59.46571","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"########## evaluating random forest model for test dataset ##########\ny_pred = rf_clf.predict(X_test_encoded)\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC curve for Random Forest Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_roc_curve\nax=py.gca()\nrfc=plot_roc_curve(rf_clf,X_test_encoded,y_test,ax=ax,alpha=0.8)\npy.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\n**Random forest** model gives the highest accuracy of all these models on both train and test dataset."},{"metadata":{},"cell_type":"markdown","source":"## E. Test Set Prediction:\n"},{"metadata":{"papermill":{"duration":0.056814,"end_time":"2020-12-07T06:58:00.323884","exception":false,"start_time":"2020-12-07T06:58:00.26707","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"######### importing test data set for evaluation ################\nX_test_evaluation = pd.read_csv('../input/loan-dataset/test_x.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_evaluation.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing data in the test data set, so no imputation is required but we need to add dummy variables for categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"########### getting dummy values for categorical columns #########\nX_test_evaluation_new = X_test_evaluation.drop(columns=\"ID_Test\")\nX_test_evaluation_new= pd.get_dummies(X_test_evaluation_new,drop_first=True)\nX_test_evaluation_new.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_evaluation_new.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##changing the order of columns in prediction test dataset same as training dataset\n\nX_test_evaluation_new = X_test_evaluation_new[['Expense','Income','Score1','Score2','Score3','Score4','Score5','Loan type_B','Occupation type_Y','Occupation type_Z','Age']]\n\nX_test_evaluation_new.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_evaluation_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######### predicting outputs using Random Forest Classifier ############\npred_y_new =rf_clf.predict(X_test_evaluation_new)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########## creating the ID of Loan Test data in a separate dataframe #######\nID_column =pd.DataFrame(X_test_evaluation[\"ID_Test\"])\n\n############ creating the final required pred_y file with ID_Test and Label_Test as columns#######\npred_y = ID_column.copy()\npred_y[\"Label_Test\"]= pred_y_new\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######### getting output file in csv format ###########\npred_y.to_csv('pred_y.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## F. Additional Understanding:"},{"metadata":{},"cell_type":"markdown","source":"### Principal Component Analysis for Standardized Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### PCA analysis for capturing 99 percent of variance in dataset ##############\nfrom sklearn.decomposition import PCA\npca = PCA(0.99)  ########## capturing 99 percent variance in dataset\n\npr_comp=pca.fit_transform(X_standard)\npr_df= pd.DataFrame([])\npr_df = pd.DataFrame(data = pr_comp,columns = ['Principal_Comp_1','Principal_Comp_2','Principal_Comp_3','Principal_Comp_4','Principal_Comp_5','Principal_Comp_6','Principal_comp_7','Principal_comp_8','Principal_comp_9'])  \npr_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############ finding percentage of information captured by each principal components\n\nprincipal_components =['Principal_Comp_1','Principal_Comp_2','Principal_Comp_3','Principal_Comp_4','Principal_Comp_5','Principal_Comp_6','Principal_comp_7','Principal_comp_8','Principal_comp_9']\nprincipal_information_percent = pd.DataFrame([])\nprincipal_information_percent = pd.DataFrame(principal_components)\nprincipal_information_percent['percent variation captured'] = pd.DataFrame(data = pca.explained_variance_ratio_)\n\nprincipal_information_percent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Principal Component Analysis need 9 components for capturing 99 percent variation in dataset. Our original dataset has 11 features, we are able to reduce only 2 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"###Finding the amount of variance explained by each principal component###\nprint(pca.explained_variance_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Directions of Principal Axes##\nprint(pca.components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### PCA analysis for capturing 95 percent of variance in dataset ##############\npca2 = PCA(0.95)  ########## capturing 95 percent variance in dataset\n\npr_comp2=pca2.fit_transform(X_standard)\npr_df2 = pd.DataFrame([])\npr_df2 = pd.DataFrame(data = pr_comp2,columns = ['New Principal_Comp_1','New Principal_Comp_2','New Princi_Comp_3','New Principal_Comp_4','New Principal_Comp_5','New Principal_Comp_6'])  \npr_df2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that, For 95 percent variance retention, there are 6 principal components, while there are 11 features in original data set. So, it nearly compressed half of the data features."},{"metadata":{"trusted":true},"cell_type":"code","source":"############ finding percentage of information captured by each principal components\n\nprincipal_components_2 =['New Principal_Comp_1','New Principal_Comp_2','New Principal_Comp_3','New Principal_Comp_4','New Principal_Comp_5','New Principal_Comp_6']\nprincipal_information_percent_2 = pd.DataFrame([])\nprincipal_information_percent_2 = pd.DataFrame(principal_components_2)\nprincipal_information_percent_2['percent variation captured'] = pd.DataFrame(data = pca2.explained_variance_ratio_)\n\nprincipal_information_percent_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing these with the 99% variance retention case, we see that the last three principal components in the 99% case are disregarded in 95% case because they have very less values of captured variance."},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Mutual information (MI) Criterion##\nfrom sklearn.feature_selection import mutual_info_classif\nmi = mutual_info_classif(X_standard, y_new,random_state = 42)\nprint(mi)\nprint(\"Mean value of MI = \", np.mean(mi))\nprint(\"Standard deviation of MI is =\", np.std(mi))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observing the mean and standard deviation, we select 0.045 as a threshold. We see that the last four features, i.e, Loan_B, Occupation_Y, Occupation_Z, Age_1.0 and Score_3 have less values of mutual information than the threshold. So, these features are less important using MI criterion."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\ntree_model = ExtraTreesClassifier(random_state = 42)\ntree_model.fit(X_standard, y_new)\nimportance_list = tree_model.feature_importances_\nprint(importance_list)\nprint(\"Mean value of importance = \", np.mean(importance_list))\nprint(\"Standard deviation of importance is =\", np.std(importance_list))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observing the mean and standard deviation, we select 0.08 as a threshold. We see that the last four features, i.e, Loan_B, Occupation_Y, Occupation_Z, and Age_1.0 have less values of importance than the threshold. So, these features are less important using Tree importance criterion."},{"metadata":{},"cell_type":"markdown","source":"So, from the intersection of both above criterion of feature importance, we can say that the last four features i.e., Loan_B, Occupation_Y, Occupation_Z, and Age_1.0 have less importance. In the original data set given in problem, these correspond to Loan type, Occupation and Age of customer. So, we can say that these are less important features than the others we have here."},{"metadata":{},"cell_type":"markdown","source":"-----------------------------------------##THANKYOU##-------------------------------------------"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}